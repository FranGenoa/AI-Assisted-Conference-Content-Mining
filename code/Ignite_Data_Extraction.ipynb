{"cells":[{"cell_type":"code","source":["\"\"\"Ignite 2025 All-in-One Downloader + Fabric Transcript Analyzer\n","\n","Combines the logic from:\n","- `Download_All_Slides.ipynb` (all sessions metadata + slides)\n","- `Download_Fabric_Videos.ipynb` (Fabric-session filtering + en-US VTT captions + Fabric OpenAI GPT analysis)\n","\n","Intended to run inside a Microsoft Fabric notebook attached to a Lakehouse.\n","\n","Notes:\n","- Parquet writing requires `pandas` + a parquet engine (e.g., `pyarrow`).\n","- Transcript analysis requires Fabric `synapse.ml.fabric` packages and a deployed model.\n","\"\"\"\n","\n","# =============================================================================\n","# CONFIGURATION - Edit these values before running\n","# =============================================================================\n","\n","# Lakehouse output path\n","LAKEHOUSE_FILES_PATH = \"/lakehouse/default/Files/Ignite2025_All\"\n","\n","# Filter keywords: set to None or [] for ALL sessions, or list keywords to filter\n","# Examples:\n","#   FILTER_KEYWORDS = None                              # Download ALL sessions\n","#   FILTER_KEYWORDS = []                                # Download ALL sessions  \n","#   FILTER_KEYWORDS = [\"fabric\", \"lakehouse\"]          # Only Fabric-related\n","#   FILTER_KEYWORDS = [\"azure ai\", \"copilot\"]          # Only AI-related\n","#   FILTER_KEYWORDS = [\"power bi\", \"semantic model\"]   # Only Power BI-related\n","FILTER_KEYWORDS = None  # <-- Change this to filter sessions\n","\n","# Download options\n","MAX_WORKERS_SLIDES = 3      # Parallel workers for PPTX downloads\n","SKIP_SLIDES = False         # Set True to skip slide deck downloads\n","SKIP_CAPTIONS = False       # Set True to skip VTT caption downloads  \n","SKIP_ANALYSIS = False       # Set True to skip transcript analysis\n","\n","# Analysis options\n","MAX_SESSIONS_ANALYZE = None  # Set to a number (e.g., 5) to limit analysis for testing\n","DEPLOYMENT_NAME = \"gpt-5\"    # Fabric OpenAI deployment name\n","API_VERSION = \"2024-08-01-preview\"  # Fabric OpenAI API version\n","TRANSCRIPT_CHAR_LIMIT = 100_000  # Truncate transcripts longer than this\n","\n","# Resume capability - automatically saves progress and resumes from failures\n","RESUME_FROM_EXISTING = True  # Skip sessions that were successfully analyzed (disable to reprocess all)\n","RETRY_FAILED = True  # Retry sessions that previously had errors\n","MAX_RETRIES_PER_SESSION = 2  # Max retry attempts per failed session\n","INITIAL_RETRY_DELAY = 2.0  # Initial delay between retries (seconds)\n","BASE_DELAY_BETWEEN_REQUESTS = 0.3  # Base delay between successful requests\n","SAVE_PROGRESS_EVERY = 10  # Save results to disk every N sessions (prevents data loss)\n","\n","# =============================================================================\n","# END CONFIGURATION\n","# =============================================================================\n","\n","from __future__ import annotations\n","\n","import json\n","import os\n","import re\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from dataclasses import dataclass\n","from datetime import datetime\n","from typing import Any, Dict, List, Optional, Tuple\n","\n","import requests\n","\n","\n","IGNITE_API = \"https://api-v2.ignite.microsoft.com/api/session/all/en-US\"\n","\n","\n","@dataclass(frozen=True)\n","class Paths:\n","    base: str\n","\n","    @property\n","    def slides_dir(self) -> str:\n","        return f\"{self.base}/slides\"\n","\n","    @property\n","    def metadata_dir(self) -> str:\n","        return f\"{self.base}/metadata\"\n","\n","    @property\n","    def captions_dir(self) -> str:\n","        return f\"{self.base}/english_captions\"\n","\n","    @property\n","    def analysis_dir(self) -> str:\n","        return f\"{self.base}/analysis\"\n","\n","\n","def _http_headers() -> Dict[str, str]:\n","    return {\n","        \"Content-Type\": \"application/json\",\n","        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n","    }\n","\n","\n","def fetch_all_sessions(timeout_s: int = 60) -> List[Dict[str, Any]]:\n","    print(\"üì° Fetching Ignite 2025 sessions...\")\n","    resp = requests.get(IGNITE_API, headers=_http_headers(), timeout=timeout_s)\n","    resp.raise_for_status()\n","    sessions = resp.json()\n","    if not isinstance(sessions, list):\n","        raise ValueError(f\"Unexpected Ignite API response type: {type(sessions).__name__}\")\n","    print(f\"‚úÖ Retrieved {len(sessions)} sessions\")\n","    return sessions\n","\n","\n","def save_json(path: str, data: Any, *, ensure_ascii: bool = False) -> None:\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(data, f, indent=2, ensure_ascii=ensure_ascii)\n","\n","\n","_INVALID_FILENAME_CHARS = re.compile(r\"[^A-Za-z0-9 _\\-\\.]+\")\n","\n","\n","def slugify_filename(text: str, *, max_len: int = 120) -> str:\n","    \"\"\"Make a filesystem-friendly slug (conservative, cross-platform).\"\"\"\n","    text = (text or \"\").strip()\n","    if not text:\n","        return \"untitled\"\n","    text = _INVALID_FILENAME_CHARS.sub(\"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    text = text.replace(\" \", \"-\")\n","    return text[:max_len].rstrip(\"- \") or \"untitled\"\n","\n","\n","def make_session_basename(session_code: str, title: str) -> str:\n","    code = (session_code or \"\").strip() or \"unknown\"\n","    title_slug = slugify_filename(title)\n","    return f\"{code}__{title_slug}\"\n","\n","\n","def safe_get(obj: Any, *keys: Any, default: Any = None) -> Any:\n","    for key in keys:\n","        if isinstance(obj, dict):\n","            obj = obj.get(key, default)\n","        elif isinstance(obj, list) and isinstance(key, int) and len(obj) > key:\n","            obj = obj[key]\n","        else:\n","            return default\n","    return obj if obj is not None else default\n","\n","\n","def extract_session_metadata(session: Dict[str, Any]) -> Dict[str, Any]:\n","    # Extract speaker names - API provides comma-separated string in 'speakerNames'\n","    speaker_names_str = safe_get(session, \"speakerNames\", default=\"\") or \"\"\n","    \n","    # Parse speaker names into structured format\n","    speakers = []\n","    if speaker_names_str:\n","        # Create list of speaker dictionaries from comma-separated string\n","        speakers = [\n","            {\"fullName\": name.strip(), \"title\": \"\", \"company\": \"\"} \n","            for name in speaker_names_str.split(\",\") if name.strip()\n","        ]\n","    \n","    # Extract products from tags field (product field is empty in API)\n","    tags = safe_get(session, \"tags\", default=[]) or []\n","    products = []\n","    if tags:\n","        # Tags are in format [{'displayValue': 'AI', 'logicalValue': 'AI'}, ...]\n","        products = [tag.get('displayValue', tag.get('logicalValue', '')) for tag in tags if isinstance(tag, dict)]\n","    \n","    # Handle sessionType - can be object with displayValue/logicalValue or string\n","    session_type = safe_get(session, \"sessionType\", default=\"\")\n","    if isinstance(session_type, dict):\n","        session_type = session_type.get(\"displayValue\", session_type.get(\"logicalValue\", \"\"))\n","    \n","    # Handle sessionLevel - API uses 'sessionLevel' array\n","    session_level_array = safe_get(session, \"sessionLevel\", default=[]) or []\n","    level = \"\"\n","    if isinstance(session_level_array, list) and len(session_level_array) > 0:\n","        first_level = session_level_array[0]\n","        if isinstance(first_level, dict):\n","            level = first_level.get(\"displayValue\", first_level.get(\"logicalValue\", \"\"))\n","    \n","    # Extract location - API uses 'location' field which combines venue/room\n","    location = safe_get(session, \"location\", default=\"\") or \"\"\n","    venue = \"\"\n","    room = \"\"\n","    if location:\n","        # Location format: \"Moscone West - Room 3004\" or just \"Moscone West\"\n","        parts = location.split(\" - \")\n","        venue = parts[0].strip()\n","        if len(parts) > 1:\n","            room = parts[1].strip()\n","    \n","    return {\n","        \"session_id\": safe_get(session, \"sessionId\", default=\"\"),\n","        \"session_code\": safe_get(session, \"sessionCode\", default=\"\"),\n","        \"title\": safe_get(session, \"title\", default=\"\"),\n","        \"description\": safe_get(session, \"description\", default=\"\"),\n","        \"level\": level,\n","        \"session_type\": session_type,\n","        \"duration_minutes\": safe_get(session, \"durationInMinutes\", default=0),\n","        \"start_time\": safe_get(session, \"startDateTime\", default=\"\"),\n","        \"end_time\": safe_get(session, \"endDateTime\", default=\"\"),\n","        \"speakers\": speakers,\n","        \"speaker_names\": speaker_names_str,\n","        \"tags\": products,\n","        \"topics\": safe_get(session, \"topic\", default=\"\"),\n","        \"learning_path\": safe_get(session, \"learningPath\", default=[]),\n","        \"slide_deck_url\": safe_get(session, \"slideDeck\", default=\"\"),\n","        \"has_slides\": bool(safe_get(session, \"slideDeck\", default=\"\")),\n","        \"video_url\": safe_get(session, \"onDemand\", default=\"\"),\n","        \"has_video\": bool(safe_get(session, \"onDemand\", default=\"\")),\n","        \"captions_url\": safe_get(session, \"captionFileLink\", default=\"\"),\n","        \"location\": location,\n","        \"venue\": venue,\n","        \"room\": room,\n","        \"extracted_at\": datetime.now().isoformat(),\n","    }\n","\n","\n","def save_metadata_outputs(paths: Paths, sessions_metadata: List[Dict[str, Any]]) -> str:\n","    \"\"\"Save ONLY the structured session metadata as JSON.\"\"\"\n","    json_path = f\"{paths.metadata_dir}/sessions_metadata.json\"\n","    save_json(json_path, sessions_metadata, ensure_ascii=False)\n","    print(f\"üíæ Saved JSON metadata: {json_path}\")\n","    return json_path\n","\n","\n","def download_file(url: str, filepath: str, session_code: str, timeout_s: int = 120) -> Dict[str, Any]:\n","    try:\n","        if os.path.exists(filepath):\n","            return {\"session_code\": session_code, \"status\": \"exists\", \"path\": filepath}\n","\n","        resp = requests.get(url, headers=_http_headers(), stream=True, timeout=timeout_s)\n","        resp.raise_for_status()\n","\n","        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n","        with open(filepath, \"wb\") as f:\n","            for chunk in resp.iter_content(chunk_size=8192):\n","                if chunk:\n","                    f.write(chunk)\n","\n","        return {\"session_code\": session_code, \"status\": \"downloaded\", \"path\": filepath}\n","\n","    except Exception as e:\n","        return {\"session_code\": session_code, \"status\": \"failed\", \"error\": str(e)}\n","\n","\n","def download_all_slides(paths: Paths, sessions_metadata: List[Dict[str, Any]], max_workers: int) -> List[Dict[str, Any]]:\n","    slides_dir = paths.slides_dir\n","    os.makedirs(slides_dir, exist_ok=True)\n","\n","    sessions_with_slides = [s for s in sessions_metadata if s.get(\"has_slides\")]\n","    print(f\"üì• Downloading {len(sessions_with_slides)} slide decks (workers={max_workers})\")\n","\n","    tasks: List[Tuple[str, str, str]] = []\n","    for session in sessions_with_slides:\n","        code = session.get(\"session_code\") or session.get(\"session_id\") or \"unknown\"\n","        title = session.get(\"title\") or \"\"\n","        basename = make_session_basename(code, title)\n","        url = session.get(\"slide_deck_url\") or \"\"\n","        if not url:\n","            continue\n","        filepath = f\"{slides_dir}/{basename}.pptx\"\n","        tasks.append((url, filepath, code))\n","\n","    results: List[Dict[str, Any]] = []\n","    downloaded = failed = existed = 0\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {executor.submit(download_file, url, path, code): code for url, path, code in tasks}\n","        for future in as_completed(futures):\n","            result = future.result()\n","            results.append(result)\n","            status = result.get(\"status\")\n","            if status == \"downloaded\":\n","                downloaded += 1\n","            elif status == \"exists\":\n","                existed += 1\n","            else:\n","                failed += 1\n","                print(f\"   ‚ùå Slide failed: {result.get('session_code')} - {result.get('error', 'Unknown error')}\")\n","\n","            total_processed = downloaded + existed + failed\n","            if total_processed % 50 == 0:\n","                print(f\"   üìä Slides progress: {total_processed}/{len(tasks)}\")\n","\n","    print(\"‚úÖ Slides complete\")\n","    print(f\"   Downloaded: {downloaded}\")\n","    print(f\"   Already existed: {existed}\")\n","    print(f\"   Failed: {failed}\")\n","\n","    return results\n","\n","\n","def extract_video_metadata(session: Dict[str, Any]) -> Dict[str, Any]:\n","    video_id = None\n","    video_url = session.get(\"onDemand\", \"\") or \"\"\n","    if video_url:\n","        match = re.search(r\"/video-nc/([a-f0-9-]+)\", video_url)\n","        if match:\n","            video_id = match.group(1)\n","\n","    return {\n","        \"session_code\": session.get(\"sessionCode\", \"\") or \"\",\n","        \"title\": session.get(\"title\", \"\") or \"\",\n","        \"description\": session.get(\"description\", \"\") or \"\",\n","        \"video_url\": video_url,\n","        \"video_id\": video_id,\n","        \"has_video\": bool(video_url),\n","    }\n","\n","\n","def extract_medius_caption_urls(embed_url: str, timeout_s: int = 30) -> List[Dict[str, str]]:\n","    \"\"\"Extract caption URLs from the Medius embed page.\"\"\"\n","    try:\n","        resp = requests.get(embed_url, headers=_http_headers(), timeout=timeout_s)\n","        resp.raise_for_status()\n","        html = resp.text\n","\n","        captions: List[Dict[str, str]] = []\n","        caption_matches = re.findall(\n","            r'\"StreamUrl\"\\s*:\\s*\"(https://mediusdl\\.event\\.microsoft\\.com[^\"]+\\.vtt[^\"]*)\"',\n","            html,\n","        )\n","        for cap_url in caption_matches:\n","            cap_url = cap_url.replace(\"\\\\u0026\", \"&\")\n","            lang_match = re.search(r\"Caption_([a-z]{2}-[A-Z]{2})\\.vtt\", cap_url)\n","            lang = lang_match.group(1) if lang_match else \"unknown\"\n","            captions.append({\"url\": cap_url, \"language\": lang})\n","\n","        return captions\n","\n","    except Exception:\n","        return []\n","\n","\n","def download_en_us_captions(paths: Paths, sessions_metadata: List[Dict[str, Any]]) -> Tuple[List[str], List[str]]:\n","    os.makedirs(paths.captions_dir, exist_ok=True)\n","\n","    sessions_with_video = [s for s in sessions_metadata if s.get(\"has_video\") and s.get(\"video_id\")]\n","    print(f\"üì• Downloading en-US VTT captions for {len(sessions_with_video)} sessions\")\n","\n","    downloaded: List[str] = []\n","    errors: List[str] = []\n","\n","    for i, session in enumerate(sessions_with_video, 1):\n","        code = session.get(\"session_code\") or \"\"\n","        if not code:\n","            errors.append(\"(missing session_code)\")\n","            continue\n","\n","        title = session.get(\"title\") or \"\"\n","        basename = make_session_basename(code, title)\n","        vtt_path = f\"{paths.captions_dir}/{basename}_en-US.vtt\"\n","        if os.path.exists(vtt_path):\n","            downloaded.append(code)\n","            if i % 10 == 0:\n","                print(f\"   [{i}/{len(sessions_with_video)}] {len(downloaded)} downloaded, {len(errors)} errors...\")\n","            continue\n","\n","        embed_url = session.get(\"video_url\") or \"\"\n","        if not embed_url:\n","            errors.append(code)\n","            continue\n","\n","        captions = extract_medius_caption_urls(embed_url)\n","        en_us_caption = next((c for c in captions if c.get(\"language\") == \"en-US\"), None)\n","        if not en_us_caption:\n","            errors.append(code)\n","            if i % 10 == 0:\n","                print(f\"   [{i}/{len(sessions_with_video)}] {len(downloaded)} downloaded, {len(errors)} errors...\")\n","            continue\n","\n","        try:\n","            resp = requests.get(en_us_caption[\"url\"], headers=_http_headers(), timeout=30)\n","            if resp.status_code == 200 and \"WEBVTT\" in resp.text:\n","                with open(vtt_path, \"w\", encoding=\"utf-8\") as f:\n","                    f.write(resp.text)\n","                downloaded.append(code)\n","            else:\n","                errors.append(code)\n","        except Exception:\n","            errors.append(code)\n","\n","        if i % 10 == 0:\n","            print(f\"   [{i}/{len(sessions_with_video)}] {len(downloaded)} downloaded, {len(errors)} errors...\")\n","\n","    print(f\"‚úÖ Captions complete: {len(downloaded)} downloaded, {len(errors)} errors\")\n","    if errors and len(errors) <= 5:\n","        print(f\"   Failed session codes: {', '.join(errors)}\")\n","\n","    return downloaded, errors\n","\n","\n","def load_existing_results(results_path: str) -> Tuple[set[str], set[str], List[Dict[str, Any]]]:\n","    \"\"\"Load existing results and return sets of successful/failed codes plus full results.\n","    \n","    Returns:\n","        (successful_codes, failed_codes, full_results)\n","    \"\"\"\n","    successful: set[str] = set()\n","    failed: set[str] = set()\n","    full_results: List[Dict[str, Any]] = []\n","    \n","    if not os.path.exists(results_path):\n","        print(f\"‚ÑπÔ∏è  No existing results file found: {results_path}\")\n","        print(\"   Starting fresh analysis\")\n","        return successful, failed, full_results\n","    \n","    try:\n","        with open(results_path, \"r\", encoding=\"utf-8\") as f:\n","            full_results = json.load(f)\n","        \n","        for result in full_results:\n","            if isinstance(result, dict):\n","                code = result.get(\"session_code\", \"\")\n","                if code:\n","                    if \"error\" in result:\n","                        failed.add(code)\n","                    else:\n","                        successful.add(code)\n","        \n","        print(f\"üìä Loaded existing results from previous run:\")\n","        print(f\"   ‚úÖ {len(successful)} successful sessions\")\n","        print(f\"   ‚ùå {len(failed)} failed sessions\")\n","        print(f\"   üì¶ {len(full_results)} total records\")\n","        \n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Could not load existing results: {e}\")\n","        print(\"   Starting fresh analysis\")\n","    \n","    return successful, failed, full_results\n","\n","\n","def parse_vtt_file(vtt_path: str) -> str:\n","    with open(vtt_path, \"r\", encoding=\"utf-8\") as f:\n","        content = f.read()\n","\n","    content = re.sub(r\"^WEBVTT.*?\\n\\n\", \"\", content, flags=re.DOTALL)\n","\n","    captions: List[str] = []\n","    for block in content.strip().split(\"\\n\\n\"):\n","        lines = block.strip().split(\"\\n\")\n","        text_lines: List[str] = []\n","        for line in lines:\n","            if \"-->\" not in line and line.strip():\n","                clean_line = re.sub(r\"<[^>]+>\", \"\", line)\n","                text_lines.append(clean_line.strip())\n","        if text_lines:\n","            captions.append(\" \".join(text_lines))\n","\n","    return \" \".join(captions)\n","\n","\n","def get_openai_config(deployment_name: str, api_version: str) -> Tuple[str, Dict[str, str]]:\n","    \"\"\"Get OpenAI configuration from environment.\n","\n","    Raises if required packages are unavailable.\n","    \"\"\"\n","    from synapse.ml.fabric.service_discovery import get_fabric_env_config  # type: ignore\n","    from synapse.ml.fabric.token_utils import TokenUtils  # type: ignore\n","\n","    fabric_env_config = get_fabric_env_config().fabric_env_config\n","    auth_header = TokenUtils().get_openai_auth_header()\n","\n","    openai_base_host = fabric_env_config.ml_workload_endpoint + \"cognitive/openai/openai/\"\n","    service_url = f\"{openai_base_host}deployments/{deployment_name}/chat/completions?api-version={api_version}\"\n","\n","    headers = {\n","        \"Authorization\": auth_header,\n","        \"Content-Type\": \"application/json\",\n","    }\n","\n","    return service_url, headers\n","\n","\n","SYSTEM_PROMPT = \"\"\"You are an expert analyst for Microsoft Ignite session transcripts.\n","Analyze and extract structured information.\n","Respond ONLY with valid JSON:\n","{\n","    \\\"summary\\\": \\\"2-3 sentence summary\\\",\n","    \\\"key_topics\\\": [\\\"topic1\\\", \\\"topic2\\\"],\n","    \\\"microsoft_features_mentioned\\\": [\\\"feature1\\\"],\n","    \\\"new_announcements\\\": [\\\"announcement1\\\"],\n","    \\\"demos_described\\\": [\\\"demo1\\\"],\n","    \\\"best_practices\\\": [\\\"practice1\\\"],\n","    \\\"target_audience\\\": \\\"description\\\",\n","    \\\"technical_level\\\": \\\"beginner|intermediate|advanced\\\",\n","    \\\"key_quotes\\\": [\\\"quote1\\\"],\n","    \\\"action_items\\\": [\\\"action1\\\"]\n","}\"\"\"\n","\n","\n","def analyze_vtt_with_openai(\n","    *,\n","    service_url: str,\n","    headers: Dict[str, str],\n","    session_code: str,\n","    session_title: str,\n","    vtt_path: str,\n","    transcript_char_limit: int,\n","    timeout_s: int = 120,\n",") -> Dict[str, Any]:\n","    transcript = parse_vtt_file(vtt_path)\n","    if len(transcript) > transcript_char_limit:\n","        transcript = transcript[:transcript_char_limit] + \"\\n\\n[Truncated...]\"\n","\n","    payload = {\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n","            {\n","                \"role\": \"user\",\n","                \"content\": f\"Session: {session_title}\\nCode: {session_code}\\n\\nTranscript:\\n{transcript}\",\n","            },\n","        ]\n","    }\n","\n","    resp = requests.post(service_url, headers=headers, json=payload, timeout=timeout_s)\n","    resp.raise_for_status()\n","\n","    response_data = resp.json()\n","    content = response_data[\"choices\"][0][\"message\"][\"content\"]\n","\n","    try:\n","        result = json.loads(content)\n","    except json.JSONDecodeError:\n","        json_match = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", content, re.DOTALL)\n","        if json_match:\n","            result = json.loads(json_match.group(1))\n","        else:\n","            result = {\"raw_response\": content, \"error\": \"Failed to parse JSON\"}\n","\n","    result[\"session_code\"] = session_code\n","    result[\"session_title\"] = session_title\n","    result[\"analyzed_at\"] = datetime.now().isoformat()\n","\n","    return result\n","\n","\n","def analyze_all_vtt_files(\n","    *,\n","    paths: Paths,\n","    sessions_metadata_path: str,\n","    deployment_name: str,\n","    api_version: str,\n","    max_sessions_analyze: Optional[int],\n","    transcript_char_limit: int,\n","    resume_from_existing: bool,\n","    retry_failed: bool,\n","    max_retries: int,\n","    initial_retry_delay: float,\n","    base_delay: float,\n","    save_progress_every: int,\n",") -> List[Dict[str, Any]]:\n","    \"\"\"Analyze VTT files with smart resume and incremental progress saving.\n","    \n","    This function:\n","    1. Loads any existing results from previous runs\n","    2. Skips already-successful sessions \n","    3. Retries previously-failed sessions (if retry_failed=True)\n","    4. Saves progress incrementally every N sessions\n","    5. Merges new results with existing ones\n","    6. Automatically refreshes auth token every 30 minutes to prevent 401 errors\n","    \"\"\"\n","    # Token refresh configuration\n","    TOKEN_REFRESH_INTERVAL = 30 * 60  # 30 minutes in seconds\n","    \n","    def get_fresh_credentials():\n","        \"\"\"Get fresh OpenAI credentials and return (service_url, headers, timestamp).\"\"\"\n","        url, hdrs = get_openai_config(deployment_name, api_version)\n","        return url, hdrs, time.time()\n","    \n","    service_url, headers, token_obtained_at = get_fresh_credentials()\n","    print(f\"üîë Authentication token obtained (auto-refresh every 30 minutes)\")\n","\n","    # Determine results file path\n","    results_json_path = f\"{paths.analysis_dir}/sessions_analysis_full.json\"\n","    \n","    # Load existing results (if resume is enabled)\n","    existing_results: List[Dict[str, Any]] = []\n","    successful_codes: set[str] = set()\n","    failed_codes: set[str] = set()\n","    \n","    if resume_from_existing:\n","        successful_codes, failed_codes, existing_results = load_existing_results(results_json_path)\n","    \n","    # Determine which sessions to skip\n","    skip_codes: set[str] = set()\n","    if resume_from_existing and not retry_failed:\n","        # Skip both successful and failed\n","        skip_codes = successful_codes | failed_codes\n","        print(f\"‚è≠Ô∏è  Skipping {len(skip_codes)} already-processed sessions (successful + failed)\")\n","    elif resume_from_existing and retry_failed:\n","        # Skip only successful, retry failed\n","        skip_codes = successful_codes\n","        print(f\"‚è≠Ô∏è  Skipping {len(skip_codes)} successful sessions\")\n","        print(f\"üîÑ Will retry {len(failed_codes)} failed sessions\")\n","\n","    # Find VTT files to process\n","    vtt_files: Dict[str, str] = {}\n","    for filename in os.listdir(paths.captions_dir):\n","        if not filename.endswith(\"_en-US.vtt\"):\n","            continue\n","        base = filename[: -len(\"_en-US.vtt\")]\n","        session_code = base.split(\"__\", 1)[0]\n","        \n","        # Skip if in exclude set\n","        if session_code in skip_codes:\n","            continue\n","            \n","        vtt_files[session_code] = f\"{paths.captions_dir}/{filename}\"\n","\n","    if max_sessions_analyze is not None:\n","        vtt_files = dict(list(vtt_files.items())[: max_sessions_analyze])\n","    \n","    if not vtt_files:\n","        print(\"‚úÖ No VTT files to process - all sessions already completed!\")\n","        return existing_results if existing_results else []\n","\n","    # Load session titles from JSON\n","    session_titles: Dict[str, str] = {}\n","    if os.path.exists(sessions_metadata_path):\n","        try:\n","            with open(sessions_metadata_path, \"r\", encoding=\"utf-8\") as f:\n","                sessions = json.load(f)\n","            for session in sessions:\n","                if isinstance(session, dict):\n","                    session_titles[session.get(\"session_code\", \"\")] = session.get(\"title\", \"\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Could not load sessions JSON: {e}\")\n","\n","    os.makedirs(paths.analysis_dir, exist_ok=True)\n","\n","    print(f\"üîÑ Analyzing {len(vtt_files)} VTT transcripts with OpenAI...\")\n","    print(f\"   Base delay between requests: {base_delay}s\")\n","    print(f\"   Max retries per session: {max_retries}\")\n","    print()\n","\n","    new_results: List[Dict[str, Any]] = []\n","    successful = 0\n","    failed = 0\n","\n","    for i, (code, vtt_path) in enumerate(vtt_files.items(), 1):\n","        title = session_titles.get(code, code)\n","        is_retry = code in failed_codes\n","        retry_label = \" (RETRY)\" if is_retry else \"\"\n","        print(f\"   [{i}/{len(vtt_files)}] {code}{retry_label}...\", end=\" \", flush=True)\n","\n","        # Check if token needs refresh (every 30 minutes)\n","        time_since_token = time.time() - token_obtained_at\n","        if time_since_token >= TOKEN_REFRESH_INTERVAL:\n","            print(f\"\\nüîÑ Refreshing authentication token (last refresh: {int(time_since_token/60)} minutes ago)...\")\n","            service_url, headers, token_obtained_at = get_fresh_credentials()\n","            print(f\"‚úÖ Token refreshed! Continuing analysis...\")\n","            print(f\"   [{i}/{len(vtt_files)}] {code}{retry_label}...\", end=\" \", flush=True)\n","\n","        # Try with exponential backoff\n","        last_error = None\n","        for attempt in range(max_retries):\n","            try:\n","                result = analyze_vtt_with_openai(\n","                    service_url=service_url,\n","                    headers=headers,\n","                    session_code=code,\n","                    session_title=title,\n","                    vtt_path=vtt_path,\n","                    transcript_char_limit=transcript_char_limit,\n","                )\n","                new_results.append(result)\n","                successful += 1\n","                level = result.get('technical_level', 'N/A')\n","                retry_info = f\" (attempt {attempt + 1})\" if attempt > 0 else \"\"\n","                print(f\"‚úÖ {level}{retry_info}\")\n","                last_error = None\n","                break  # Success!\n","                \n","            except Exception as e:\n","                last_error = e\n","                if attempt < max_retries - 1:\n","                    # Exponential backoff\n","                    delay = initial_retry_delay * (2 ** attempt)\n","                    print(f\"‚ö†Ô∏è  Attempt {attempt + 1} failed, retrying in {delay}s...\", end=\" \", flush=True)\n","                    time.sleep(delay)\n","                else:\n","                    # Final failure\n","                    error_msg = str(e)[:60]\n","                    print(f\"‚ùå Failed after {max_retries} attempts: {error_msg}\")\n","        \n","        # If all retries failed, record the error\n","        if last_error is not None:\n","            new_results.append({\n","                \"session_code\": code,\n","                \"session_title\": title,\n","                \"error\": str(last_error),\n","                \"attempts\": max_retries,\n","                \"analyzed_at\": datetime.now().isoformat()\n","            })\n","            failed += 1\n","\n","        # Delay between requests\n","        time.sleep(base_delay)\n","        \n","        # Save progress incrementally to prevent data loss\n","        if (i % save_progress_every == 0) or (i == len(vtt_files)):\n","            # Merge current progress with existing results\n","            processed_codes = {r[\"session_code\"] for r in new_results}\n","            kept_existing = [r for r in existing_results if r.get(\"session_code\") not in processed_codes]\n","            current_full_results = kept_existing + new_results\n","            \n","            # Save to disk\n","            with open(results_json_path, \"w\", encoding=\"utf-8\") as f:\n","                json.dump(current_full_results, f, indent=2, ensure_ascii=False)\n","            \n","            if i < len(vtt_files):\n","                print(f\"   üíæ Progress saved ({len(current_full_results)} total sessions)\")\n","\n","    print()\n","    print(f\"‚úÖ Analysis complete! {successful} successful, {failed} failed\")\n","    print()\n","    \n","    # Final merge already done during incremental saves - just get the latest\n","    final_results = new_results\n","    if resume_from_existing and existing_results:\n","        processed_codes = {r[\"session_code\"] for r in new_results}\n","        kept_existing = [r for r in existing_results if r.get(\"session_code\") not in processed_codes]\n","        final_results = kept_existing + new_results\n","        print(f\"üì¶ Final database: {len(kept_existing)} existing + {len(new_results)} new = {len(final_results)} total\")\n","    else:\n","        print(f\"üì¶ Final database: {len(final_results)} sessions\")\n","    \n","    # Save final Parquet\n","    try:\n","        import pandas as pd\n","\n","        # Flatten lists in results for parquet compatibility\n","        flattened: List[Dict[str, Any]] = []\n","        for r in final_results:\n","            flat = r.copy()\n","            for key, value in flat.items():\n","                if isinstance(value, list):\n","                    flat[key] = json.dumps(value)\n","            flattened.append(flat)\n","\n","        df = pd.DataFrame(flattened)\n","        parquet_path = f\"{paths.analysis_dir}/sessions_analysis.parquet\"\n","        df.to_parquet(parquet_path, index=False)\n","        print(f\"üíæ Saved Parquet: {parquet_path}\")\n","\n","    except ImportError:\n","        print(\"‚ö†Ô∏è  pandas not available - skipping Parquet output\")\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è  Failed to write analysis Parquet: {e}\")\n","    \n","    # JSON already saved incrementally, just confirm final version is written\n","    with open(results_json_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(final_results, f, indent=2, ensure_ascii=False)\n","    print(f\"üíæ Saved JSON: {results_json_path}\")\n","    \n","    print()\n","    print(f\"üéØ Analysis Summary:\")\n","    print(f\"   This run: {successful} successful, {failed} failed\")\n","    print(f\"   Total in database: {len(final_results)} sessions\")\n","\n","    return final_results\n","\n","\n","def write_download_report(paths: Paths, sessions_metadata: List[Dict[str, Any]], slide_results: List[Dict[str, Any]]) -> str:\n","    report = {\n","        \"download_date\": datetime.now().isoformat(),\n","        \"total_sessions\": len(sessions_metadata),\n","        \"sessions_with_slides\": sum(1 for s in sessions_metadata if s.get(\"has_slides\")),\n","        \"sessions_with_video\": sum(1 for s in sessions_metadata if s.get(\"has_video\")),\n","        \"slides_downloaded\": sum(1 for r in slide_results if r.get(\"status\") == \"downloaded\"),\n","        \"slides_already_existed\": sum(1 for r in slide_results if r.get(\"status\") == \"exists\"),\n","        \"slides_failed\": sum(1 for r in slide_results if r.get(\"status\") == \"failed\"),\n","        \"failed_sessions\": [r for r in slide_results if r.get(\"status\") == \"failed\"],\n","        \"output_directory\": paths.base,\n","    }\n","\n","    report_path = f\"{paths.base}/download_report.json\"\n","    save_json(report_path, report, ensure_ascii=False)\n","    return report_path\n","\n","\n","def run():\n","    \"\"\"Main execution function - uses configuration variables defined at the top.\"\"\"\n","    \n","    paths = Paths(base=LAKEHOUSE_FILES_PATH)\n","    for d in [paths.metadata_dir, paths.slides_dir, paths.captions_dir, paths.analysis_dir]:\n","        os.makedirs(d, exist_ok=True)\n","\n","    print(f\"üìÅ Output directory: {LAKEHOUSE_FILES_PATH}\")\n","    \n","    sessions = fetch_all_sessions()\n","\n","    # Build filter function based on FILTER_KEYWORDS config\n","    filter_keywords = FILTER_KEYWORDS\n","    if filter_keywords is not None and len(filter_keywords) == 0:\n","        filter_keywords = None\n","\n","    def session_matches_filter(session: Dict[str, Any]) -> bool:\n","        \"\"\"Return True if session matches any filter keyword, or if no filter is set.\"\"\"\n","        if filter_keywords is None:\n","            return True\n","        title = (session.get(\"title\") or \"\").lower()\n","        description = (session.get(\"description\") or \"\").lower()\n","        products = \" \".join([str(p).lower() for p in (session.get(\"products\") or [])])\n","        searchable = f\"{title} {description} {products}\"\n","        return any(kw.lower() in searchable for kw in filter_keywords)\n","\n","    # Apply filter to raw sessions for downstream processing\n","    if filter_keywords:\n","        filtered_sessions = [s for s in sessions if session_matches_filter(s)]\n","        print(f\"üîç Filter applied: {filter_keywords}\")\n","        print(f\"   Matched {len(filtered_sessions)} of {len(sessions)} sessions\")\n","    else:\n","        filtered_sessions = sessions\n","        print(\"üîç No filter applied ‚Äî processing all sessions\")\n","\n","    # Extract metadata for ALL sessions (for full metadata file)\n","    all_sessions_metadata = [extract_session_metadata(s) for s in sessions]\n","    print(\"üîÑ Extracted structured metadata for all sessions\")\n","    print(f\"   Total sessions: {len(all_sessions_metadata)}\")\n","    print(f\"   Sessions with slides: {sum(1 for s in all_sessions_metadata if s.get('has_slides'))}\")\n","    print(f\"   Sessions with video: {sum(1 for s in all_sessions_metadata if s.get('has_video'))}\")\n","\n","    save_metadata_outputs(paths, all_sessions_metadata)\n","\n","    # For downloads, use filtered sessions\n","    filtered_metadata = [extract_session_metadata(s) for s in filtered_sessions]\n","\n","    slide_results: List[Dict[str, Any]] = []\n","    if not SKIP_SLIDES:\n","        slide_results = download_all_slides(paths, filtered_metadata, max_workers=MAX_WORKERS_SLIDES)\n","    else:\n","        print(\"‚è≠Ô∏è Skipping slide downloads (SKIP_SLIDES=True)\")\n","\n","    report_path = write_download_report(paths, all_sessions_metadata, slide_results)\n","    print(f\"üìä Saved download report: {report_path}\")\n","\n","    if SKIP_CAPTIONS:\n","        print(\"‚è≠Ô∏è Skipping VTT caption downloads (SKIP_CAPTIONS=True)\")\n","        return\n","\n","    # For VTT captions: apply the same filter\n","    filtered_video_metadata = [extract_video_metadata(s) for s in filtered_sessions]\n","\n","    filtered_json_path = f\"{paths.metadata_dir}/filtered_sessions.json\"\n","    save_json(filtered_json_path, filtered_video_metadata, ensure_ascii=False)\n","\n","    print(f\"üéØ Filtered sessions for VTT download: {len(filtered_video_metadata)}\")\n","    print(f\"   With video: {sum(1 for s in filtered_video_metadata if s.get('has_video'))}\")\n","    print(f\"üíæ Saved filtered session metadata: {filtered_json_path}\")\n","\n","    download_en_us_captions(paths, filtered_video_metadata)\n","\n","    if SKIP_ANALYSIS:\n","        print(\"‚è≠Ô∏è Skipping transcript analysis (SKIP_ANALYSIS=True)\")\n","        return\n","\n","    # Transcript analysis requires packages\n","    try:\n","        analyze_all_vtt_files(\n","            paths=paths,\n","            sessions_metadata_path=filtered_json_path,\n","            deployment_name=DEPLOYMENT_NAME,\n","            api_version=API_VERSION,\n","            max_sessions_analyze=MAX_SESSIONS_ANALYZE,\n","            transcript_char_limit=TRANSCRIPT_CHAR_LIMIT,\n","            resume_from_existing=RESUME_FROM_EXISTING,\n","            retry_failed=RETRY_FAILED,\n","            max_retries=MAX_RETRIES_PER_SESSION,\n","            initial_retry_delay=INITIAL_RETRY_DELAY,\n","            base_delay=BASE_DELAY_BETWEEN_REQUESTS,\n","            save_progress_every=SAVE_PROGRESS_EVERY,\n","        )\n","\n","    except ModuleNotFoundError as e:\n","        print(\n","            \"‚ùå Transcript analysis requires synapse.ml packages. \"\n","            \"Run this inside Microsoft Fabric, or set SKIP_ANALYSIS=True.\\n\"\n","            f\"Details: {e}\"\n","        )\n","\n","    print(\"\\n‚úÖ All done!\")\n","\n","\n","# Run the script\n","run()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"8fac7315-83b9-4590-9289-f7923ede01b3","normalized_state":"finished","queued_time":"2025-12-14T08:39:40.2023334Z","session_start_time":null,"execution_start_time":"2025-12-14T08:39:40.2034615Z","execution_finish_time":"2025-12-14T08:39:44.9392772Z","parent_msg_id":"02bc7611-bed2-4821-a8e2-7ee771b717fc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üìÅ Output directory: /lakehouse/default/Files/Ignite2025_All\nüì° Fetching Ignite 2025 sessions...\n‚úÖ Retrieved 1090 sessions\nüîç No filter applied ‚Äî processing all sessions\nüîÑ Extracted structured metadata for all sessions\n   Total sessions: 1090\n   Sessions with slides: 289\n   Sessions with video: 502\nüíæ Saved JSON metadata: /lakehouse/default/Files/Ignite2025_All/metadata/sessions_metadata.json\nüì• Downloading 289 slide decks (workers=3)\n   üìä Slides progress: 50/289\n   üìä Slides progress: 100/289\n   üìä Slides progress: 150/289\n   üìä Slides progress: 200/289\n   üìä Slides progress: 250/289\n‚úÖ Slides complete\n   Downloaded: 0\n   Already existed: 289\n   Failed: 0\nüìä Saved download report: /lakehouse/default/Files/Ignite2025_All/download_report.json\nüéØ Filtered sessions for VTT download: 1090\n   With video: 502\nüíæ Saved filtered session metadata: /lakehouse/default/Files/Ignite2025_All/metadata/filtered_sessions.json\nüì• Downloading en-US VTT captions for 502 sessions\n   [10/502] 10 downloaded, 0 errors...\n   [20/502] 20 downloaded, 0 errors...\n   [30/502] 30 downloaded, 0 errors...\n   [40/502] 40 downloaded, 0 errors...\n   [50/502] 49 downloaded, 1 errors...\n   [60/502] 59 downloaded, 1 errors...\n   [70/502] 69 downloaded, 1 errors...\n   [80/502] 79 downloaded, 1 errors...\n   [90/502] 89 downloaded, 1 errors...\n   [100/502] 99 downloaded, 1 errors...\n   [110/502] 109 downloaded, 1 errors...\n   [120/502] 119 downloaded, 1 errors...\n   [130/502] 129 downloaded, 1 errors...\n   [140/502] 139 downloaded, 1 errors...\n   [150/502] 149 downloaded, 1 errors...\n   [160/502] 158 downloaded, 2 errors...\n   [170/502] 168 downloaded, 2 errors...\n   [180/502] 178 downloaded, 2 errors...\n   [190/502] 188 downloaded, 2 errors...\n   [200/502] 198 downloaded, 2 errors...\n   [210/502] 208 downloaded, 2 errors...\n   [220/502] 218 downloaded, 2 errors...\n   [230/502] 228 downloaded, 2 errors...\n   [240/502] 238 downloaded, 2 errors...\n   [250/502] 248 downloaded, 2 errors...\n   [260/502] 258 downloaded, 2 errors...\n   [270/502] 268 downloaded, 2 errors...\n   [280/502] 278 downloaded, 2 errors...\n   [290/502] 287 downloaded, 3 errors...\n   [300/502] 297 downloaded, 3 errors...\n   [310/502] 307 downloaded, 3 errors...\n   [320/502] 317 downloaded, 3 errors...\n   [330/502] 327 downloaded, 3 errors...\n   [340/502] 337 downloaded, 3 errors...\n   [350/502] 347 downloaded, 3 errors...\n   [360/502] 356 downloaded, 4 errors...\n   [370/502] 366 downloaded, 4 errors...\n   [380/502] 376 downloaded, 4 errors...\n   [390/502] 386 downloaded, 4 errors...\n   [400/502] 396 downloaded, 4 errors...\n   [410/502] 406 downloaded, 4 errors...\n   [420/502] 416 downloaded, 4 errors...\n   [430/502] 426 downloaded, 4 errors...\n   [440/502] 436 downloaded, 4 errors...\n   [450/502] 446 downloaded, 4 errors...\n   [460/502] 456 downloaded, 4 errors...\n   [470/502] 466 downloaded, 4 errors...\n   [480/502] 476 downloaded, 4 errors...\n   [490/502] 486 downloaded, 4 errors...\n   [500/502] 496 downloaded, 4 errors...\n‚úÖ Captions complete: 498 downloaded, 4 errors\n   Failed session codes: BRK123, BRK264, BRK217, BRK236\nüîë Authentication token obtained (auto-refresh every 30 minutes)\nüìä Loaded existing results from previous run:\n   ‚úÖ 498 successful sessions\n   ‚ùå 0 failed sessions\n   üì¶ 498 total records\n‚è≠Ô∏è  Skipping 498 successful sessions\nüîÑ Will retry 0 failed sessions\n‚úÖ No VTT files to process - all sessions already completed!\n\n‚úÖ All done!\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"70705902-207e-4627-b968-ff3753d3c874"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"99682b45-ce3b-4a5e-9273-93070a87ecdf"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"0f99be1b-e2d7-4eb0-8c68-0091f9022eeb","known_lakehouses":[{"id":"0f99be1b-e2d7-4eb0-8c68-0091f9022eeb"}],"default_lakehouse_name":"NewLakehouse","default_lakehouse_workspace_id":"af9b4eda-f3be-49b1-bac1-bf685cff4e27"}}},"nbformat":4,"nbformat_minor":5}